{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdTzXjglMaHH3oAU34tSea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadow-Walker360/Shadow-Walker360/blob/main/aviator_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "RJEBqZk0q_SK",
        "outputId": "7c29feeb-90b4-464f-b215-09c1ea822e3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'requests' from 'google.colab' (/usr/local/lib/python3.10/dist-packages/google/colab/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-caa0192a60f4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'requests' from 'google.colab' (/usr/local/lib/python3.10/dist-packages/google/colab/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "BASE_URL = 'https://www.betika.com/en-ke/aviator'\n",
        "ROUNDS = 100\n",
        "THRESHOLD = 1.5\n",
        "AUTH_TOKEN = 'your_auth_token'\n",
        "BET_AMOUNT = 100\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger('aviator_game')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create a rotating file handler\n",
        "handler = RotatingFileHandler('aviator_game.log', maxBytes=1000000, backupCount=1)\n",
        "handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create a logging format\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the logger\n",
        "logger.addHandler(handler)\n",
        "\n",
        "def discover_urls(base_url):\n",
        "    \"\"\"\n",
        "    Discover data and bet URLs automatically from the base URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        data_url = data.get('data_url')\n",
        "        bet_url = data.get('bet_url')\n",
        "        return data_url, bet_url\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f'Error discovering URLs: {e}')\n",
        "        return None, None\n",
        "\n",
        "def collect_data(data_url, rounds):\n",
        "    \"\"\"\n",
        "    Collect game data for a specified number of rounds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(data_url, params={'rounds': rounds})\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f'Error collecting data: {e}')\n",
        "        return None\n",
        "\n",
        "def train_model(df):\n",
        "    \"\"\"\n",
        "    Train a Random Forest model with hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        features = ['crash_point_lag_1', 'crash_point_lag_2', 'crash_point_lag_3', 'crash_point_lag_4']\n",
        "        target = 'crash_point'\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "        # Hyperparameter tuning using Grid Search\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 5, 10],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 5, 10]\n",
        "        }\n",
        "\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_model.fit(X_train, y_train)\n",
        "\n",
        "        return best_model\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error training model: {e}')\n",
        "        return None\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model using Mean Absolute Error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y_pred = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        return mae\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error evaluating model: {e}')\n",
        "        return None\n",
        "\n",
        "def predict_next_crash_point(model, lagged_points):\n",
        "    \"\"\"\n",
        "    Predict the next crash point based on lagged data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return model.predict([lagged_points])[0]\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error predicting next crash point: {e}')\n",
        "        return None\n",
        "\n",
        "def should_place_bet(predicted_crash_point, threshold):\n",
        "    \"\"\"\n",
        "    Decide whether to place a bet based on the predicted crash point.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return predicted_crash_point >= threshold\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error determining whether to place bet: {e}')\n",
        "        return False\n",
        "\n",
        "def place_bet(amount, predicted_crash_point, bet_url, auth_token):\n",
        "    \"\"\"\n",
        "    Place a bet using the Aviator game API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'Authorization': f'Bearer {auth_token}',\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        bet_data = {\n",
        "            'amount': amount,\n",
        "            'predicted_crash_point': predicted_crash_point\n",
        "        }\n",
        "\n",
        "        response = requests.post(bet_url, headers=headers, data=json.dumps(bet_data))\n",
        "        response.raise_for_status()\n",
        "        logger.info(f'Bet placed successfully: {response.json()}')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f'Error placing bet: {e}')\n",
        "\n",
        "def visualize_data(df):\n",
        "    \"\"\"\n",
        "    Visualize the crash point trend.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(df['crash_point'], label='Crash Point')\n",
        "        plt.axhline(y=THRESHOLD, color='r', linestyle='--', label='Bet Threshold')\n",
        "        plt.title('Crash Point Trend')\n",
        "        plt.xlabel('Round')\n",
        "        plt.ylabel('Crash Point')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error visualizing data: {e}')\n",
        "\n",
        "def main():\n",
        "    # Discover URLs\n",
        "    data_url, bet_url = discover_urls(BASE_URL)\n",
        "    if data_url is None or bet_url is None:\n",
        "        logger.error('Failed to discover URLs. Exiting.')\n",
        "        return\n",
        "\n",
        "    # Collect data\n",
        "    df = collect_data(data_url, ROUNDS)\n",
        "    if df is None:\n",
        "        logger.error('Failed to collect data. Exiting.')\n",
        "        return\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(df)\n",
        "    if model is None:\n",
        "        logger.error('Failed to train model. Exiting.')\n",
        "        return\n",
        "\n",
        "    # Prepare data for evaluation\n",
        "    features = ['crash_point_lag_1', 'crash_point_lag_2', 'crash_point_lag_3', 'crash_point_lag_4']\n",
        "    X_test = df[features].iloc[-20:]  # Last 20 rounds for testing\n",
        "    y_test = df['crash_point'].iloc[-20:]\n",
        "\n",
        "    # Evaluate model\n",
        "    mae = evaluate_model(model, X_test, y_test)\n",
        "    if mae is not None:\n",
        "        logger.info(f'Model Evaluation - Mean Absolute Error: {mae}')\n",
        "\n",
        "    # Predict next crash point\n",
        "    last_lagged_points = df[features].iloc[-1].values\n",
        "    predicted_crash_point = predict_next_crash_point(model, last_lagged_points)\n",
        "    logger.info(f'Predicted Next Crash Point: {predicted_crash_point}')\n",
        "\n",
        "    # Decide whether to place a bet\n",
        "    if should_place_bet(predicted_crash_point, THRESHOLD):\n",
        "        place_bet(BET_AMOUNT, predicted_crash_point, bet_url, AUTH_TOKEN)\n",
        "    else:\n",
        "        logger.info('No bet placed. Predicted crash point below threshold.')\n",
        "\n",
        "    # Visualize data\n",
        "    visualize_data(df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}