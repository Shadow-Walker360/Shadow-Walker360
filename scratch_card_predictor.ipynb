{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtbHkCaAHR3DDMldPAXC7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadow-Walker360/Shadow-Walker360/blob/main/scratch_card_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzJkzpmfPzjs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import time\n",
        "\n",
        "# Set up logging to file\n",
        "logging.basicConfig(filename='model_training.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Function to generate synthetic scratch card data\n",
        "def generate_scratch_card_data(num_samples=10000, card_length=10):\n",
        "    try:\n",
        "        cards = []\n",
        "        targets = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            # Generate random card numbers (e.g., between 0 and 9)\n",
        "            card = np.random.randint(0, 10, size=card_length).tolist()\n",
        "            cards.append(''.join(map(str, card)))  # Convert to string for card representation\n",
        "            # The target can be something like the sum of the card's numbers\n",
        "            target = sum(card)  # This can be changed to something more complex\n",
        "            targets.append(target)\n",
        "\n",
        "        data = pd.DataFrame({\n",
        "            'card': cards,\n",
        "            'target': targets\n",
        "        })\n",
        "\n",
        "        logging.info(f\"Dataset generated successfully with {num_samples} samples.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in generating scratch card data: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to preprocess and train models\n",
        "def train_models(data):\n",
        "    try:\n",
        "        # Preprocessing - Convert 'card' into a numeric format\n",
        "        data['card'] = data['card'].apply(lambda x: [int(i) for i in x])  # Convert string to list of integers\n",
        "        X = np.array(data['card'].tolist())\n",
        "        y = np.array(data['target'])\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        logging.info(\"Data preprocessing completed. Splitting data into training and test sets.\")\n",
        "\n",
        "        # Model 1: RandomForestRegressor\n",
        "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "        # Model 2: XGBRegressor\n",
        "        xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "        xgb_predictions = xgb_model.predict(X_test)\n",
        "\n",
        "        logging.info(\"Models trained successfully.\")\n",
        "\n",
        "        # Evaluate models\n",
        "        rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "        rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "        xgb_mae = mean_absolute_error(y_test, xgb_predictions)\n",
        "        xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
        "\n",
        "        logging.info(f\"Random Forest - MAE: {rf_mae}, MSE: {rf_mse}\")\n",
        "        logging.info(f\"XGBoost - MAE: {xgb_mae}, MSE: {xgb_mse}\")\n",
        "\n",
        "        # Hyperparameter optimization for RandomForestRegressor using GridSearchCV\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "        grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=3)\n",
        "        grid_search_rf.fit(X_train, y_train)\n",
        "        best_rf_model = grid_search_rf.best_estimator_\n",
        "        logging.info(f\"Best RandomForest model: {grid_search_rf.best_params_}\")\n",
        "\n",
        "        # Hyperparameter optimization for XGBRegressor\n",
        "        param_grid_xgb = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "        grid_search_xgb = GridSearchCV(XGBRegressor(), param_grid_xgb, cv=3)\n",
        "        grid_search_xgb.fit(X_train, y_train)\n",
        "        best_xgb_model = grid_search_xgb.best_estimator_\n",
        "        logging.info(f\"Best XGBoost model: {grid_search_xgb.best_params_}\")\n",
        "\n",
        "        # Final evaluation after hyperparameter optimization\n",
        "        best_rf_predictions = best_rf_model.predict(X_test)\n",
        "        best_xgb_predictions = best_xgb_model.predict(X_test)\n",
        "\n",
        "        best_rf_mae = mean_absolute_error(y_test, best_rf_predictions)\n",
        "        best_rf_mse = mean_squared_error(y_test, best_rf_predictions)\n",
        "\n",
        "        best_xgb_mae = mean_absolute_error(y_test, best_xgb_predictions)\n",
        "        best_xgb_mse = mean_squared_error(y_test, best_xgb_predictions)\n",
        "\n",
        "        logging.info(f\"Optimized Random Forest - MAE: {best_rf_mae}, MSE: {best_rf_mse}\")\n",
        "        logging.info(f\"Optimized XGBoost - MAE: {best_xgb_mae}, MSE: {best_xgb_mse}\")\n",
        "\n",
        "        return best_rf_model, best_xgb_model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model training and evaluation: {e}\")\n",
        "        raise\n",
        "\n",
        "# Main function to run the entire process\n",
        "def main():\n",
        "    try:\n",
        "        logging.info(\"Starting the model training process.\")\n",
        "\n",
        "        # Step 1: Generate the dataset\n",
        "        data = generate_scratch_card_data(num_samples=10000, card_length=10)\n",
        "\n",
        "        # Step 2: Train models and evaluate\n",
        "        best_rf_model, best_xgb_model = train_models(data)\n",
        "\n",
        "        logging.info(\"Model training and evaluation completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main function: {e}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}